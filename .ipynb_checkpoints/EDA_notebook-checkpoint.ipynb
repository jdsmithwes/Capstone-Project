{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import nltk\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import word2vec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec model creation\n",
    "\n",
    "#Covidtweet_selective_model = gensim.models.Word2Vec(tok_corp, size=100, window=10, min_count=500, workers=4)\n",
    "#Covidtweet_selective_model.save('Covidtweet_model_7.20')\n",
    "\n",
    "#loading Word2Vec\n",
    "Covidtweet_model = gensim.models.Word2Vec.load(\"Covidtweet_model_7.20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2Vec Vocabulary\n",
    "tweet_vocab = Covidtweet_model.wv.vocab\n",
    "words = len(tweet_vocab)\n",
    "widget_input = list(tweet_vocab)\n",
    "print(f'There are {words} words in this Word2Vec vocabulary.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 50 Tokens In Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top tokens\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "\n",
    "fig= plt.figure(figsize=(20,5))\n",
    "plt.xticks(rotation=25)\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "docs = vectorizer.fit_transform(clean_tweet)\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "visualizer = FreqDistVisualizer(features=features, orient='v')\n",
    "visualizer.fit(docs)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#closest word graph\n",
    "from tweet_functions import *\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "interact(display_closestwords_tsnescatterplot,model=fixed(Covidtweet_model),word=widget_input,size=fixed(100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "common_dictionary = Dictionary(tok_corp)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in tok_corp]\n",
    "\n",
    "lda = LdaModel(common_corpus, num_topics=20, alpha='auto', eval_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "list_of_list_of_tokens = list(tok_corp)\n",
    "dictionary_LDA = corpora.Dictionary(list_of_list_of_tokens)\n",
    "dictionary_LDA.filter_extremes(no_below=3)\n",
    "corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in list_of_list_of_tokens]\n",
    "\n",
    "num_topics = 20\n",
    "%time lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "                                  id2word=dictionary_LDA, \\\n",
    "                                  passes=4, alpha=[0.01]*num_topics, \\\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 15\n",
    "for i,topic in lda_model.show_topics(formatted=True, num_topics=num_topics, num_words=10):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(topic_model=lda_model, corpus=corpus, dictionary=dictionary_LDA)\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, string, collections\n",
    "from nltk.util import ngrams # function for making ngrams\n",
    "#from nltk import bigrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting trigrams  \n",
    "vectorizer = CountVectorizer(ngram_range = (3,3)) \n",
    "X1 = vectorizer.fit_transform(clean_tweet)  \n",
    "features = (vectorizer.get_feature_names()) \n",
    "#print(\"\\n\\nFeatures : \\n\", features) \n",
    "#print(\"\\n\\nX1 : \\n\", X1.toarray()) \n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range = (3,3)) \n",
    "X2 = vectorizer.fit_transform(clean_tweet) \n",
    "scores = (X2.toarray()) \n",
    "#print(\"\\n\\nScores : \\n\", scores) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = X2.sum(axis = 0) \n",
    "data1 = [] \n",
    "for col, term in enumerate(features): \n",
    "    data1.append( (term, sums[0,col] )) \n",
    "ranking = pd.DataFrame(data1, columns = ['term','rank']) \n",
    "words = (ranking.sort_values('rank', ascending = False)) \n",
    "print (\"\\n\\nWords head : \\n\", words.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
