{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZaLJG7wss5zY"
   },
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import textfeatures as tf\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import PatternAnalyzer, NaiveBayesAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jDKGyLuus5zd"
   },
   "source": [
    "## Obtaining Tweet Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTTYzZqes5zd"
   },
   "source": [
    "Tweet data gathered in the notebook for Tweet data queries are put into dataframes from their original JSON format. This will assist with adding COVID and Poll Data later for full analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXOlmeqns5ze"
   },
   "outputs": [],
   "source": [
    "#All Covid tweets\n",
    "All_Covid_tweets = pd.read_json('tweets/Covid_tweets5.json',lines=True)\n",
    "\n",
    "#All Trump tweets\n",
    "Trump_Covid_tweets = pd.read_json('tweets/Trump_Covid_tweets4.json', lines=True)\n",
    "\n",
    "#All Cuomo tweets\n",
    "Cuomo_Covid_tweets = pd.read_json('tweets/Cuomo_Covid_tweets5.json',lines=True)\n",
    "\n",
    "#Baseline Tweets\n",
    "NYTimes_tweets = pd.read_json('tweets/Nytimes_Covid_tweets5.json',lines=True)\n",
    "#print( len(NYTimes_tweets))\n",
    "WashingtonPost_tweets = pd.read_json('tweets/Washpost_tweets5.json',lines=True)\n",
    "#print( len(Washpost_tweets3.json))\n",
    "\n",
    "#combining NYTimes and Washington Post to get Baseline Tweets\n",
    "Baseline_tweets = pd.concat([NYTimes_tweets,WashingtonPost_tweets],axis=0)\n",
    "\n",
    "#Reformatting Date columns for later merge\n",
    "All_Covid_tweets['date'] = pd.to_datetime(All_Covid_tweets['date'], format='%Y%m%d')\n",
    "Trump_Covid_tweets['date'] = pd.to_datetime(Trump_Covid_tweets['date'], format='%Y%m%d')\n",
    "Cuomo_Covid_tweets['date'] = pd.to_datetime(Cuomo_Covid_tweets['date'], format='%Y%m%d')\n",
    "Baseline_tweets['date'] = pd.to_datetime(Baseline_tweets['date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets in the dataset is: 24681\n"
     ]
    }
   ],
   "source": [
    "#Combining all Tweet DFs into one\n",
    "Master_Tweet_df = pd.concat([All_Covid_tweets,Trump_Covid_tweets,\n",
    "                             Cuomo_Covid_tweets,Baseline_tweets],axis=0)\n",
    "\n",
    "number_of_tweets = len(Master_Tweet_df)\n",
    "print('The number of tweets in the dataset is:',number_of_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Covid Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NNW39ui7s5zh"
   },
   "source": [
    "COVID tracking data was pulled from [this repository](https://github.com/COVID19Tracking/covid-tracking-data/blob/master/data/us_daily.csv). I choose this resource because it is monitored daily and is maintained by a reputable organization - The Atlantic. While this data includes details around ICU populations and number of people on ventilators, I will be focused primarily on the number of positive cases and deathes in the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jtAcw-ss5zi"
   },
   "outputs": [],
   "source": [
    "# Covid data set\n",
    "covid_data = pd.read_csv('https://raw.githubusercontent.com/COVID19Tracking/covid-tracking-data/master/data/us_daily.csv')\n",
    "\n",
    "# formatting the date column to datetime\n",
    "covid_data['date'] = pd.to_datetime(covid_data['date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>positive</th>\n",
       "      <th>death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-25</td>\n",
       "      <td>5749803</td>\n",
       "      <td>170353.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-08-24</td>\n",
       "      <td>5713124</td>\n",
       "      <td>169206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-08-23</td>\n",
       "      <td>5678088</td>\n",
       "      <td>168863.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-08-22</td>\n",
       "      <td>5639766</td>\n",
       "      <td>168291.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-08-21</td>\n",
       "      <td>5593318</td>\n",
       "      <td>167262.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  positive     death\n",
       "0 2020-08-25   5749803  170353.0\n",
       "1 2020-08-24   5713124  169206.0\n",
       "2 2020-08-23   5678088  168863.0\n",
       "3 2020-08-22   5639766  168291.0\n",
       "4 2020-08-21   5593318  167262.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_data = covid_data[['date','positive','death']]\n",
    "covid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbbEt4Ijs5zy"
   },
   "source": [
    "### Adding Case/Death Data on Day of the Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zt7s9ym9s5z4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>timezone</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>place</th>\n",
       "      <th>...</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>translate</th>\n",
       "      <th>trans_src</th>\n",
       "      <th>trans_dest</th>\n",
       "      <th>positive</th>\n",
       "      <th>death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1228397478954795009</td>\n",
       "      <td>1228397478954795008</td>\n",
       "      <td>2020-02-14 19:15:56</td>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>14:15:56</td>\n",
       "      <td>EDT</td>\n",
       "      <td>143132365</td>\n",
       "      <td>nycemswatch</td>\n",
       "      <td>NYC EMS Watch</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[{'user_id': '143132365', 'username': 'NYCEMSw...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1228397478954795009</td>\n",
       "      <td>1228397478954795008</td>\n",
       "      <td>2020-02-14 19:15:56</td>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>14:15:56</td>\n",
       "      <td>EDT</td>\n",
       "      <td>143132365</td>\n",
       "      <td>nycemswatch</td>\n",
       "      <td>NYC EMS Watch</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[{'user_id': '143132365', 'username': 'NYCEMSw...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1228397478954795009</td>\n",
       "      <td>1228397478954795008</td>\n",
       "      <td>2020-02-14 19:15:56</td>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>14:15:56</td>\n",
       "      <td>EDT</td>\n",
       "      <td>143132365</td>\n",
       "      <td>nycemswatch</td>\n",
       "      <td>NYC EMS Watch</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[{'user_id': '143132365', 'username': 'NYCEMSw...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1231683951896383488</td>\n",
       "      <td>1231683951896383488</td>\n",
       "      <td>2020-02-23 20:55:12</td>\n",
       "      <td>2020-02-23</td>\n",
       "      <td>15:55:12</td>\n",
       "      <td>EDT</td>\n",
       "      <td>44984619</td>\n",
       "      <td>bobfoolery</td>\n",
       "      <td>Bob Cooper</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[{'user_id': '44984619', 'username': 'BobFoole...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>122</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1231683951896383488</td>\n",
       "      <td>1231683951896383488</td>\n",
       "      <td>2020-02-23 20:55:12</td>\n",
       "      <td>2020-02-23</td>\n",
       "      <td>15:55:12</td>\n",
       "      <td>EDT</td>\n",
       "      <td>44984619</td>\n",
       "      <td>bobfoolery</td>\n",
       "      <td>Bob Cooper</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[{'user_id': '44984619', 'username': 'BobFoole...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>122</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id      conversation_id          created_at       date  \\\n",
       "0  1228397478954795009  1228397478954795008 2020-02-14 19:15:56 2020-02-14   \n",
       "1  1228397478954795009  1228397478954795008 2020-02-14 19:15:56 2020-02-14   \n",
       "2  1228397478954795009  1228397478954795008 2020-02-14 19:15:56 2020-02-14   \n",
       "3  1231683951896383488  1231683951896383488 2020-02-23 20:55:12 2020-02-23   \n",
       "4  1231683951896383488  1231683951896383488 2020-02-23 20:55:12 2020-02-23   \n",
       "\n",
       "       time timezone    user_id     username           name place  ...  \\\n",
       "0  14:15:56      EDT  143132365  nycemswatch  NYC EMS Watch        ...   \n",
       "1  14:15:56      EDT  143132365  nycemswatch  NYC EMS Watch        ...   \n",
       "2  14:15:56      EDT  143132365  nycemswatch  NYC EMS Watch        ...   \n",
       "3  15:55:12      EDT   44984619   bobfoolery     Bob Cooper        ...   \n",
       "4  15:55:12      EDT   44984619   bobfoolery     Bob Cooper        ...   \n",
       "\n",
       "  user_rt_id user_rt retweet_id  \\\n",
       "0                                 \n",
       "1                                 \n",
       "2                                 \n",
       "3                                 \n",
       "4                                 \n",
       "\n",
       "                                            reply_to  retweet_date  translate  \\\n",
       "0  [{'user_id': '143132365', 'username': 'NYCEMSw...                            \n",
       "1  [{'user_id': '143132365', 'username': 'NYCEMSw...                            \n",
       "2  [{'user_id': '143132365', 'username': 'NYCEMSw...                            \n",
       "3  [{'user_id': '44984619', 'username': 'BobFoole...                            \n",
       "4  [{'user_id': '44984619', 'username': 'BobFoole...                            \n",
       "\n",
       "   trans_src trans_dest positive death  \n",
       "0                             21   0.0  \n",
       "1                             21   0.0  \n",
       "2                             21   0.0  \n",
       "3                            122   0.0  \n",
       "4                            122   0.0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging the two datasets for all Covid data in one place\n",
    "Master_Tweet_dataset = pd.merge(Master_Tweet_df,covid_data,on='date')\n",
    "Master_Tweet_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Unneeded Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns were deleted from the dataframe because they were mainly metadata that wouldn't be used in the various models that will be utilized later in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unnecessary columns\n",
    "\n",
    "Master_Tweet_dataset = Master_Tweet_dataset.drop(['cashtags', 'conversation_id', 'hashtags',\n",
    "       'id','link', 'mentions', 'name', 'near', 'photos',\n",
    "       'place', 'quote_url','reply_to', 'retweet',\n",
    "       'retweet_date', 'retweet_id','source', 'time',\n",
    "       'timezone', 'trans_dest', 'trans_src', 'translate','urls',\n",
    "       'user_id', 'user_rt', 'user_rt_id',],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lGw1vL04BvRP"
   },
   "source": [
    "### SCRUBBING OF TWEETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct most of the preprocessing of the tweet data, I utilized the Textfeatures library. This library assists with not only lowercase and remove punctuation and items such as hashtags, but provide useful data such as word count, average word length, stopwords count, and the character count of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'username', 'tweet', 'replies_count', 'retweets_count',\n",
       "       'likes_count', 'video', 'geo', 'positive', 'death'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install textfeatures\n",
    "#Getting ride of duplicative column\n",
    "Master_Tweet_dataset=Master_Tweet_dataset.drop('created_at',axis=1)\n",
    "Master_Tweet_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>video</th>\n",
       "      <th>geo</th>\n",
       "      <th>positive</th>\n",
       "      <th>death</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>stopwords_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>stopwords</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>nycemswatch</td>\n",
       "      <td>Everything you need to know about AMR right he...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "      <td>9.780488</td>\n",
       "      <td>12</td>\n",
       "      <td>442</td>\n",
       "      <td>[you, to, about, who, for, have, after, a, wit...</td>\n",
       "      <td>everything need know right emts work american ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>nycemswatch</td>\n",
       "      <td>Everything you need to know about AMR right he...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "      <td>9.780488</td>\n",
       "      <td>12</td>\n",
       "      <td>442</td>\n",
       "      <td>[you, to, about, who, for, have, after, a, wit...</td>\n",
       "      <td>everything need know right emts work american ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>nycemswatch</td>\n",
       "      <td>Everything you need to know about AMR right he...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "      <td>9.780488</td>\n",
       "      <td>12</td>\n",
       "      <td>442</td>\n",
       "      <td>[you, to, about, who, for, have, after, a, wit...</td>\n",
       "      <td>everything need know right emts work american ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-23</td>\n",
       "      <td>bobfoolery</td>\n",
       "      <td>If/when COVID-19 becomes epidemic in the Unite...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46</td>\n",
       "      <td>5.021739</td>\n",
       "      <td>15</td>\n",
       "      <td>276</td>\n",
       "      <td>[in, the, will, down, as, has, been, in, and, ...</td>\n",
       "      <td>ifwhen covid becomes epidemic united states am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-23</td>\n",
       "      <td>bobfoolery</td>\n",
       "      <td>If/when COVID-19 becomes epidemic in the Unite...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>122</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46</td>\n",
       "      <td>5.021739</td>\n",
       "      <td>15</td>\n",
       "      <td>276</td>\n",
       "      <td>[in, the, will, down, as, has, been, in, and, ...</td>\n",
       "      <td>ifwhen covid becomes epidemic united states am...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date     username                                              tweet  \\\n",
       "0 2020-02-14  nycemswatch  Everything you need to know about AMR right he...   \n",
       "1 2020-02-14  nycemswatch  Everything you need to know about AMR right he...   \n",
       "2 2020-02-14  nycemswatch  Everything you need to know about AMR right he...   \n",
       "3 2020-02-23   bobfoolery  If/when COVID-19 becomes epidemic in the Unite...   \n",
       "4 2020-02-23   bobfoolery  If/when COVID-19 becomes epidemic in the Unite...   \n",
       "\n",
       "   replies_count  retweets_count  likes_count  video geo  positive  death  \\\n",
       "0              3              12           15      0            21    0.0   \n",
       "1              3              12           15      0            21    0.0   \n",
       "2              3              12           15      0            21    0.0   \n",
       "3              0               0            0      0           122    0.0   \n",
       "4              0               0            0      0           122    0.0   \n",
       "\n",
       "   word_count  avg_word_length  stopwords_count  char_count  \\\n",
       "0          37         9.780488               12         442   \n",
       "1          37         9.780488               12         442   \n",
       "2          37         9.780488               12         442   \n",
       "3          46         5.021739               15         276   \n",
       "4          46         5.021739               15         276   \n",
       "\n",
       "                                           stopwords  \\\n",
       "0  [you, to, about, who, for, have, after, a, wit...   \n",
       "1  [you, to, about, who, for, have, after, a, wit...   \n",
       "2  [you, to, about, who, for, have, after, a, wit...   \n",
       "3  [in, the, will, down, as, has, been, in, and, ...   \n",
       "4  [in, the, will, down, as, has, been, in, and, ...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  everything need know right emts work american ...  \n",
       "1  everything need know right emts work american ...  \n",
       "2  everything need know right emts work american ...  \n",
       "3  ifwhen covid becomes epidemic united states am...  \n",
       "4  ifwhen covid becomes epidemic united states am...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using textfeatures library text preprocessing\n",
    "tf.word_count(Master_Tweet_dataset,\"tweet\",'word_count')\n",
    "tf.avg_word_length(Master_Tweet_dataset,'tweet','avg_word_length')\n",
    "tf.stopwords_count(Master_Tweet_dataset,'tweet','stopwords_count')\n",
    "tf.char_count(Master_Tweet_dataset,'tweet','char_count')\n",
    "tf.stopwords(Master_Tweet_dataset,'tweet','stopwords')\n",
    "tf.clean(Master_Tweet_dataset,'tweet','clean_text')\n",
    "Master_Tweet_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making sure all of the text data is uniform and all special characters have been removed, I utiilized the TweetTokenizer library to tokenize every tweet to form the larger corpus. Tokenizing the tweets is an important step to assist the computer make sense of the text that was gathered through the tweets. Tokenization will allow for us to ascertain the topics being discussed in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing the Corpus\n",
    "\n",
    "clean_tweet = Master_Tweet_dataset['clean_text']\n",
    "#Tweet Tokenizer \n",
    "\n",
    "ttknz = TweetTokenizer()\n",
    "\n",
    "#creation of the corpus\n",
    "#corpus = Master_Tweet_df['clean_tweets'].astype(str)\n",
    "#corpus.dtypes\n",
    "\n",
    "#tokenizing corpus\n",
    "tok_corp = []\n",
    "for sent in clean_tweet:\n",
    "    toked = ttknz.tokenize(sent)\n",
    "    tok_corp.append(toked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving tok list or later use\n",
    "\n",
    "with open('tok_corp_8_8.pickle','wb') as f:\n",
    "    pickle.dump(tok_corp,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Sentiment Analysis, the TextBlob library was utilized. The output of this analyis will give each tweet a score between -1 and 1. Scores closer to -1 can be classified as negative and scores around zero can be thought of as being neutral in sentiment. Finally, items with a score closer to 1 can be defined as having a positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GpAWXJlFHlGJ"
   },
   "outputs": [],
   "source": [
    "#Sentiment Analysis - adding Sentiment rating to each tweet\n",
    "\n",
    "#from twitter_nlp_toolkit.tweet_sentiment_classifier import tweet_sentiment_classifier\n",
    "\n",
    "#tweets = Master_Tweet_df['clean_tweets']\n",
    "\n",
    "tweets = Master_Tweet_dataset['clean_text']\n",
    "\n",
    "Sentiment = []\n",
    "for tweet in tweets:\n",
    "    #Classifier = tweet_sentiment_classifier.SentimentAnalyzer()\n",
    "    #sentiment = Classifier.predict_proba(tweet)\n",
    "    blob = TextBlob(tweet,analyzer=PatternAnalyzer())\n",
    "    rating = blob.sentiment.polarity\n",
    "    Sentiment.append(rating)\n",
    "\n",
    "Master_Tweet_dataset['Sentiment'] = Sentiment\n",
    "#Master_Tweet_df['Sentiment'] = Master_Tweet_df['Sentiment'].astype(int)\n",
    "#Master_Tweet_df['Sentiment'].round(decimals = 4)\n",
    "\n",
    "#Master_Tweet_df['Sentiment'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Poll Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polling data was collected from [RealClear Politics](https://www.realclearpolitics.com/epolls/2020/president/us/general_election_trump_vs_biden-6247.html). This source was chosen because they maintain a collection of polls for the US Presidential election and they do a good job of showing the overall momentum a candidate may have at a given time. To assist with the merge, I manipulated the date column in excel so that the merge would be successful as attempts to make the changes with Pandas were unsuccessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading Poll data\n",
    "poll_data = pd.read_csv('data/poll_data_dates.csv')\n",
    "poll_data = poll_data.dropna()\n",
    "\n",
    "#Converting Date columns to integer so merge will work\n",
    "Master_Tweet_dataset['date'] = pd.to_datetime(Master_Tweet_dataset['date'])\n",
    "Master_Tweet_dataset['date'] = Master_Tweet_dataset['date'].astype(int)\n",
    "poll_data['date'] = pd.to_datetime(poll_data['date'])\n",
    "poll_data['date'] = poll_data['date'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging with Poll Data\n",
    "\n",
    "#poll_data = pd.read_csv('data/poll_data_dates.csv')\n",
    "#pd.to_datetime(poll_data['Date']) #converting to datetime object for merge purposes\n",
    "#pd.to_datetime(Master_Tweet_df['Date']) #converting to datetime object for merge purposes\n",
    "\n",
    "left = Master_Tweet_dataset.sort_values(by='date')\n",
    "right = poll_data.sort_values(by='date')\n",
    "\n",
    "Master_Tweet_dataset = pd.merge_asof(left,right,on='date',allow_exact_matches=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Target Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, determining the public's perception of the Covid response in America was critical. To learn this based on tweets, Latent Dirichlet Allocation (LDA) was employed. LDA is an unsupervised machine-learning model that takes documents as input and finds topics as output. Further,as detailed on [Towards Data Science](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0), LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities.\n",
    "\n",
    "To generate these topics, count vectorization was used to convert the corpus of documents into a matrix of token counts which was later fed into the LDA model so that topics could be generated. As you will see, the ten topics will have the most common words for that topic and the associated percentage that word appears in that topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assiging topics to each tweet  -code inspired by stackabuse\n",
    "\n",
    "\n",
    "#Vectorizing docs\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(clean_tweet)\n",
    "\n",
    "#fitting LDA Model\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "LDA_model_8_22 = LDA.fit(doc_term_matrix)\n",
    "\n",
    "#transforming to get topic numbers\n",
    "topic_values = LDA.transform(doc_term_matrix)\n",
    "topic_values.shape\n",
    "\n",
    "#creating column of Topics\n",
    "Master_Tweet_dataset['Topic'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['blood', 'government', 'make', 'virus', 'people', 'trump', 'coronavirus', 'american', 'covid', 'pandemic']\n",
      "Top 10 words for topic #1:\n",
      "['thousands', 'trumps', 'people', 'death', 'deaths', 'coronavirus', 'trump', 'pandemic', 'covid', 'american']\n",
      "Top 10 words for topic #2:\n",
      "['deaths', 'people', 'trumpdeathclock', 'delayed', 'cost', 'coronavirus', 'trumps', 'lives', 'american', 'covid']\n",
      "Top 10 words for topic #3:\n",
      "['compared', 'disease', 'americans', 'people', 'countries', 'health', 'coronavirus', 'pandemic', 'american', 'covid']\n",
      "Top 10 words for topic #4:\n",
      "['customers', 'gave', 'service', 'especially', 'provide', 'family', 'relief', 'america', 'american', 'covid']\n",
      "Top 10 words for topic #5:\n",
      "['black', 'free', 'time', 'social', 'fund', 'support', 'nurses', 'pandemic', 'american', 'covid']\n",
      "Top 10 words for topic #6:\n",
      "['delta', 'coronavirus', 'lines', 'including', 'pandemic', 'united', 'states', 'airlines', 'american', 'covid']\n",
      "Top 10 words for topic #7:\n",
      "['crisis', 'emergency', 'health', 'fund', 'world', 'coronavirus', 'support', 'pandemic', 'covid', 'american']\n",
      "Top 10 words for topic #8:\n",
      "['bounties', 'soldiers', 'trumps', 'americans', 'deaths', 'like', 'people', 'trump', 'covid', 'american']\n",
      "Top 10 words for topic #9:\n",
      "['said', 'care', 'trump', 'health', 'medical', 'coronavirus', 'people', 'pandemic', 'american', 'covid']\n"
     ]
    }
   ],
   "source": [
    "# top words for each topic\n",
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary to create Target Column\n",
    "category_dict = {}\n",
    "for key in [0,4,5,8,1,3]:\n",
    "    category_dict[key] = 'Bad Response'\n",
    "for key in [10,6,2,7,9]:\n",
    "    category_dict[key] = 'Good Response'\n",
    "    \n",
    "#Creation of Target Column\n",
    "Master_Tweet_dataset['Target'] = Master_Tweet_dataset['Topic'].map(category_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ready DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the data fed to our models is uniform, this last step just dropped columns that were either superflous or contained text that our number-driven models cannot make any use of. For portability, the final dataframe was pickled so that it could be recalled in the Models notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Master_Tweet_dataset = Master_Tweet_dataset.drop(['date','username','video','geo','clean_text','Start Date', \n",
    "                                                  'End Date','tweet','Sample','replies_count', 'retweets_count',\n",
    "                                                  'MoE','stopwords','Poll','MoE'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes_count</th>\n",
       "      <th>positive</th>\n",
       "      <th>death</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>stopwords_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Biden (D)</th>\n",
       "      <th>Trump (R)</th>\n",
       "      <th>Spread</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>909</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34</td>\n",
       "      <td>5.484848</td>\n",
       "      <td>12</td>\n",
       "      <td>215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Biden +6</td>\n",
       "      <td>2</td>\n",
       "      <td>Good Response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>909</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34</td>\n",
       "      <td>5.484848</td>\n",
       "      <td>12</td>\n",
       "      <td>215</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Biden +6</td>\n",
       "      <td>2</td>\n",
       "      <td>Good Response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>79</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>50.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Biden +6</td>\n",
       "      <td>2</td>\n",
       "      <td>Good Response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>79</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>2</td>\n",
       "      <td>124</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>50.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Biden +6</td>\n",
       "      <td>7</td>\n",
       "      <td>Good Response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37</td>\n",
       "      <td>9.780488</td>\n",
       "      <td>12</td>\n",
       "      <td>442</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>50.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>Biden +7</td>\n",
       "      <td>7</td>\n",
       "      <td>Good Response</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   likes_count  positive  death  word_count  avg_word_length  stopwords_count  \\\n",
       "0          909         6    0.0          34         5.484848               12   \n",
       "1          909         6    0.0          34         5.484848               12   \n",
       "2           79         7    0.0          16         7.200000                2   \n",
       "3           79         7    0.0          16         7.200000                2   \n",
       "4           15        21    0.0          37         9.780488               12   \n",
       "\n",
       "   char_count  Sentiment  Biden (D)  Trump (R)    Spread  Topic         Target  \n",
       "0         215   0.000000       50.0       44.0  Biden +6      2  Good Response  \n",
       "1         215   0.000000       50.0       44.0  Biden +6      2  Good Response  \n",
       "2         124   0.136364       50.0       44.0  Biden +6      2  Good Response  \n",
       "3         124   0.136364       50.0       44.0  Biden +6      7  Good Response  \n",
       "4         442   0.071429       50.0       43.0  Biden +7      7  Good Response  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Master_Tweet_dataset['death'] = Master_Tweet_dataset['death'].fillna(0)\n",
    "Master_Tweet_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(Master_Tweet_dataset,open('Master_Tweet_dataset.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24681"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Master_Tweet_dataset['Target'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Covid_notebook_7.14.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
