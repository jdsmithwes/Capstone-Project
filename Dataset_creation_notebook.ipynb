{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZaLJG7wss5zY"
   },
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import textfeatures as tf\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jDKGyLuus5zd"
   },
   "source": [
    "## Creating Dataframe with Tweet Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qTTYzZqes5zd"
   },
   "source": [
    "Tweet data gathered in the notebook for Tweet data queries are put into dataframes from their original JSON format. This will assist with adding COVID and Poll Data later for full analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXOlmeqns5ze"
   },
   "outputs": [],
   "source": [
    "#All Covid tweets\n",
    "All_Covid_tweets = pd.read_json('tweets/Covid_tweets5.json',lines=True)\n",
    "\n",
    "#All Trump tweets\n",
    "Trump_Covid_tweets = pd.read_json('tweets/Trump_Covid_tweets5.json', lines=True)\n",
    "\n",
    "#All Cuomo tweets\n",
    "Cuomo_Covid_tweets = pd.read_json('tweets/Cuomo_Covid_tweets5.json',lines=True)\n",
    "\n",
    "#Baseline Tweets\n",
    "NYTimes_tweets = pd.read_json('tweets/Nytimes_Covid_tweets5.json',lines=True)\n",
    "#print( len(NYTimes_tweets))\n",
    "WashingtonPost_tweets = pd.read_json('tweets/Washpost_tweets5.json',lines=True)\n",
    "#print( len(Washpost_tweets3.json))\n",
    "\n",
    "#combining NYTimes and Washington Post to get Baseline Tweets\n",
    "Baseline_tweets = pd.concat([NYTimes_tweets,WashingtonPost_tweets],axis=0)\n",
    "\n",
    "#Reformatting Date columns for later merge\n",
    "All_Covid_tweets['date'] = pd.to_datetime(All_Covid_tweets['date'], format='%Y%m%d')\n",
    "Trump_Covid_tweets['date'] = pd.to_datetime(Trump_Covid_tweets['date'], format='%Y%m%d')\n",
    "Cuomo_Covid_tweets['date'] = pd.to_datetime(Cuomo_Covid_tweets['date'], format='%Y%m%d')\n",
    "Baseline_tweets['date'] = pd.to_datetime(Baseline_tweets['date'], format='%Y%m%d')\n",
    "\n",
    "#Saving DF for use in other notebooks\n",
    "pickle.dump(All_Covid_tweets,open('pickles/All_Covid_tweets.pickle','wb'))\n",
    "pickle.dump(Trump_Covid_tweets,open('pickles/Trump_Covid_tweets.pickle','wb'))\n",
    "pickle.dump(Cuomo_Covid_tweets,open('pickles/Cuomo_Covid_tweets.pickle','wb'))\n",
    "pickle.dump(Baseline_tweets,open('pickles/Baseline_tweets.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets in the dataset is: 27437\n"
     ]
    }
   ],
   "source": [
    "#Combining all Tweet DFs into one\n",
    "Master_Tweet_df = pd.concat([All_Covid_tweets,Trump_Covid_tweets,\n",
    "                             Cuomo_Covid_tweets,Baseline_tweets],axis=0)\n",
    "\n",
    "A = number_of_tweets = len(Master_Tweet_df)\n",
    "print('The number of tweets in the dataset is:',number_of_tweets)\n",
    "\n",
    "#Saving DF for use in other notebooks\n",
    "pickle.dump(Master_Tweet_df,open('pickles/Master_Tweet_df','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Covid Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NNW39ui7s5zh"
   },
   "source": [
    "COVID tracking data was pulled from [this repository](https://github.com/COVID19Tracking/covid-tracking-data/blob/master/data/us_daily.csv). I choose this resource because it is monitored daily and is maintained by a reputable organization - The Atlantic. While this data includes details around ICU populations and number of people on ventilators, I will be focused primarily on the number of positive cases and deathes in the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0jtAcw-ss5zi"
   },
   "outputs": [],
   "source": [
    "# Covid data set\n",
    "covid_data = pd.read_csv('https://raw.githubusercontent.com/COVID19Tracking/covid-tracking-data/master/data/us_daily.csv')\n",
    "\n",
    "# formatting the date column to datetime\n",
    "covid_data['date'] = pd.to_datetime(covid_data['date'], format='%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>positive</th>\n",
       "      <th>death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-09-08</td>\n",
       "      <td>6298422</td>\n",
       "      <td>181690.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-09-07</td>\n",
       "      <td>6276203</td>\n",
       "      <td>181332.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-09-06</td>\n",
       "      <td>6247522</td>\n",
       "      <td>181107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-09-05</td>\n",
       "      <td>6214407</td>\n",
       "      <td>180658.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-04</td>\n",
       "      <td>6169497</td>\n",
       "      <td>179740.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        date  positive     death\n",
       "0 2020-09-08   6298422  181690.0\n",
       "1 2020-09-07   6276203  181332.0\n",
       "2 2020-09-06   6247522  181107.0\n",
       "3 2020-09-05   6214407  180658.0\n",
       "4 2020-09-04   6169497  179740.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#three columns for the Covid dataframe\n",
    "covid_data = covid_data[['date','positive','death']]\n",
    "covid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lbbEt4Ijs5zy"
   },
   "source": [
    "### Adding Case/Death Data on Day of the Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zt7s9ym9s5z4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>timezone</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>place</th>\n",
       "      <th>...</th>\n",
       "      <th>user_rt_id</th>\n",
       "      <th>user_rt</th>\n",
       "      <th>retweet_id</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>retweet_date</th>\n",
       "      <th>translate</th>\n",
       "      <th>trans_src</th>\n",
       "      <th>trans_dest</th>\n",
       "      <th>positive</th>\n",
       "      <th>death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1228397478954795009</td>\n",
       "      <td>1228397478954795008</td>\n",
       "      <td>2020-02-14 19:15:56</td>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>14:15:56</td>\n",
       "      <td>EDT</td>\n",
       "      <td>143132365</td>\n",
       "      <td>nycemswatch</td>\n",
       "      <td>NYC EMS Watch</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[{'user_id': '143132365', 'username': 'NYCEMSw...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1228397478954795009</td>\n",
       "      <td>1228397478954795008</td>\n",
       "      <td>2020-02-14 19:15:56</td>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>14:15:56</td>\n",
       "      <td>EDT</td>\n",
       "      <td>143132365</td>\n",
       "      <td>nycemswatch</td>\n",
       "      <td>NYC EMS Watch</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[{'user_id': '143132365', 'username': 'NYCEMSw...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1228397478954795009</td>\n",
       "      <td>1228397478954795008</td>\n",
       "      <td>2020-02-14 19:15:56</td>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>14:15:56</td>\n",
       "      <td>EDT</td>\n",
       "      <td>143132365</td>\n",
       "      <td>nycemswatch</td>\n",
       "      <td>NYC EMS Watch</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[{'user_id': '143132365', 'username': 'NYCEMSw...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1228468353486008320</td>\n",
       "      <td>1228468353486008320</td>\n",
       "      <td>2020-02-14 23:57:33</td>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>18:57:33</td>\n",
       "      <td>EDT</td>\n",
       "      <td>25073877</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[{'user_id': '25073877', 'username': 'realDona...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1228463577335554049</td>\n",
       "      <td>1228463577335554048</td>\n",
       "      <td>2020-02-14 23:38:35</td>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>18:38:35</td>\n",
       "      <td>EDT</td>\n",
       "      <td>25073877</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>Donald J. Trump</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[{'user_id': '25073877', 'username': 'realDona...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id      conversation_id          created_at       date  \\\n",
       "0  1228397478954795009  1228397478954795008 2020-02-14 19:15:56 2020-02-14   \n",
       "1  1228397478954795009  1228397478954795008 2020-02-14 19:15:56 2020-02-14   \n",
       "2  1228397478954795009  1228397478954795008 2020-02-14 19:15:56 2020-02-14   \n",
       "3  1228468353486008320  1228468353486008320 2020-02-14 23:57:33 2020-02-14   \n",
       "4  1228463577335554049  1228463577335554048 2020-02-14 23:38:35 2020-02-14   \n",
       "\n",
       "       time timezone    user_id         username             name place  ...  \\\n",
       "0  14:15:56      EDT  143132365      nycemswatch    NYC EMS Watch        ...   \n",
       "1  14:15:56      EDT  143132365      nycemswatch    NYC EMS Watch        ...   \n",
       "2  14:15:56      EDT  143132365      nycemswatch    NYC EMS Watch        ...   \n",
       "3  18:57:33      EDT   25073877  realdonaldtrump  Donald J. Trump        ...   \n",
       "4  18:38:35      EDT   25073877  realdonaldtrump  Donald J. Trump        ...   \n",
       "\n",
       "  user_rt_id user_rt retweet_id  \\\n",
       "0                                 \n",
       "1                                 \n",
       "2                                 \n",
       "3                                 \n",
       "4                                 \n",
       "\n",
       "                                            reply_to  retweet_date  translate  \\\n",
       "0  [{'user_id': '143132365', 'username': 'NYCEMSw...                            \n",
       "1  [{'user_id': '143132365', 'username': 'NYCEMSw...                            \n",
       "2  [{'user_id': '143132365', 'username': 'NYCEMSw...                            \n",
       "3  [{'user_id': '25073877', 'username': 'realDona...                            \n",
       "4  [{'user_id': '25073877', 'username': 'realDona...                            \n",
       "\n",
       "   trans_src trans_dest positive death  \n",
       "0                              0   0.0  \n",
       "1                              0   0.0  \n",
       "2                              0   0.0  \n",
       "3                              0   0.0  \n",
       "4                              0   0.0  \n",
       "\n",
       "[5 rows x 36 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#merging the two datasets for all Covid data in one place\n",
    "Master_Tweet_dataset = pd.merge(Master_Tweet_df,covid_data,on='date')\n",
    "Master_Tweet_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Unneeded Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns were deleted from the dataframe because they were mainly metadata that wouldn't be used in the various models that will be utilized later in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unnecessary columns\n",
    "\n",
    "Master_Tweet_dataset = Master_Tweet_dataset.drop(['cashtags', 'conversation_id', 'hashtags',\n",
    "       'id','link', 'mentions', 'name', 'near', 'photos',\n",
    "       'place', 'quote_url','reply_to', 'retweet',\n",
    "       'retweet_date', 'retweet_id','source', 'time',\n",
    "       'timezone', 'trans_dest', 'trans_src', 'translate','urls',\n",
    "       'user_id', 'user_rt', 'user_rt_id',],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lGw1vL04BvRP"
   },
   "source": [
    "### SCRUBBING OF TWEETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conduct most of the preprocessing of the tweet data, I utilized the Textfeatures library. This library assists with not only lowercase and remove punctuation and items such as hashtags, but provide useful data such as word count, average word length, stopwords count, and the character count of each tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'username', 'tweet', 'replies_count', 'retweets_count',\n",
       "       'likes_count', 'video', 'geo', 'positive', 'death'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pip install textfeatures\n",
    "#Getting ride of duplicative column\n",
    "Master_Tweet_dataset=Master_Tweet_dataset.drop('created_at',axis=1)\n",
    "Master_Tweet_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using textfeatures library text preprocessing\n",
    "tf.word_count(Master_Tweet_dataset,\"tweet\",'word_count')\n",
    "tf.avg_word_length(Master_Tweet_dataset,'tweet','avg_word_length')\n",
    "tf.stopwords_count(Master_Tweet_dataset,'tweet','stopwords_count')\n",
    "tf.char_count(Master_Tweet_dataset,'tweet','char_count')\n",
    "tf.stopwords(Master_Tweet_dataset,'tweet','stopwords')\n",
    "tf.clean(Master_Tweet_dataset,'tweet','clean_text')\n",
    "Master_Tweet_dataset.head()\n",
    "\n",
    "#Saving DF for use in other notebooks\n",
    "pickle.dump(Master_Tweet_dataset,open('pickles/Master_Tweet_datasetfull.pickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making sure all of the text data is uniform and all special characters have been removed, I utiilized the TweetTokenizer library to tokenize every tweet to form the larger corpus. Tokenizing the tweets is an important step to assist the computer make sense of the text that was gathered through the tweets. Tokenization will allow for us to ascertain the topics being discussed in the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing the Corpus\n",
    "\n",
    "clean_tweet = Master_Tweet_dataset['clean_text']\n",
    "#Tweet Tokenizer \n",
    "\n",
    "ttknz = TweetTokenizer()\n",
    "\n",
    "#creation of the corpus\n",
    "#corpus = Master_Tweet_df['clean_tweets'].astype(str)\n",
    "#corpus.dtypes\n",
    "\n",
    "#tokenizing corpus\n",
    "tok_corp = []\n",
    "for sent in clean_tweet:\n",
    "    toked = ttknz.tokenize(sent)\n",
    "    tok_corp.append(toked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving tok list or later use\n",
    "\n",
    "with open('tok_corp_8_8.pickle','wb') as f:\n",
    "    pickle.dump(tok_corp,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Sentiment Analysis, the TextBlob library was utilized. The output of this analyis will give each tweet a score between -1 and 1. Scores closer to -1 can be classified as negative and scores around zero can be thought of as being neutral in sentiment. Finally, items with a score closer to 1 can be defined as having a positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GpAWXJlFHlGJ"
   },
   "outputs": [],
   "source": [
    "#Sentiment Analysis - adding Sentiment rating to each tweet\n",
    "\n",
    "#from twitter_nlp_toolkit.tweet_sentiment_classifier import tweet_sentiment_classifier\n",
    "\n",
    "#tweets = Master_Tweet_df['clean_tweets']\n",
    "\n",
    "tweets = Master_Tweet_dataset['clean_text']\n",
    "\n",
    "Sentiment = []\n",
    "for tweet in tweets:\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = list(analyzer.polarity_scores(tweet).values())\n",
    "    sentiment = vs[3]\n",
    "    Sentiment.append(sentiment)\n",
    "    \n",
    "    \n",
    "    #Classifier = tweet_sentiment_classifier.SentimentAnalyzer()\n",
    "    #sentiment = Classifier.predict_proba(tweet)\n",
    "    #blob = TextBlob(tweet,analyzer=PatternAnalyzer())\n",
    "    #rating = blob.sentiment.polarity\n",
    "    #Sentiment.append(rating)\n",
    "\n",
    "Master_Tweet_dataset['Sentiment'] = Sentiment\n",
    "#Master_Tweet_df['Sentiment'] = Master_Tweet_df['Sentiment'].astype(int)\n",
    "#Master_Tweet_df['Sentiment'].round(decimals = 4)\n",
    "\n",
    "#Master_Tweet_df['Sentiment'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Poll Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polling data was collected from [RealClear Politics](https://www.realclearpolitics.com/epolls/2020/president/us/general_election_trump_vs_biden-6247.html). This source was chosen because they maintain a collection of polls for the US Presidential election and they do a good job of showing the overall momentum a candidate may have at a given time. To assist with the merge, I manipulated the date column in excel so that the merge would be successful as attempts to make the changes with Pandas were unsuccessful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uploading Poll data\n",
    "poll_data = pd.read_csv('data/poll_data_dates.csv')\n",
    "poll_data = poll_data.dropna()\n",
    "\n",
    "#Converting Date columns to integer so merge will work\n",
    "Master_Tweet_dataset['date'] = pd.to_datetime(Master_Tweet_dataset['date'])\n",
    "Master_Tweet_dataset['date'] = Master_Tweet_dataset['date'].astype(int)\n",
    "poll_data['date'] = pd.to_datetime(poll_data['date'])\n",
    "poll_data['date'] = poll_data['date'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging with Poll Data\n",
    "\n",
    "#poll_data = pd.read_csv('data/poll_data_dates.csv')\n",
    "#pd.to_datetime(poll_data['Date']) #converting to datetime object for merge purposes\n",
    "#pd.to_datetime(Master_Tweet_df['Date']) #converting to datetime object for merge purposes\n",
    "\n",
    "left = Master_Tweet_dataset.sort_values(by='date')\n",
    "right = poll_data.sort_values(by='date')\n",
    "\n",
    "Master_Tweet_dataset = pd.merge_asof(left,right,on='date',allow_exact_matches=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of Target Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this analysis, determining the public's perception of the Covid response in America was critical. To learn this based on tweets, Latent Dirichlet Allocation (LDA) was employed. LDA is an unsupervised machine-learning model that takes documents as input and finds topics as output. Further,as detailed on [Towards Data Science](https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0), LDA is a generative probabilistic model that assumes each topic is a mixture over an underlying set of words, and each document is a mixture of over a set of topic probabilities.\n",
    "\n",
    "To generate these topics, count vectorization was used to convert the corpus of documents into a matrix of token counts which was later fed into the LDA model so that topics could be generated. As you will see, the ten topics will have the most common words for that topic and the associated percentage that word appears in that topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assiging topics to each tweet  -code inspired by stackabuse\n",
    "\n",
    "\n",
    "#Vectorizing docs\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(clean_tweet)\n",
    "\n",
    "#fitting LDA Model\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA_model_8_22 = LDA.fit(doc_term_matrix)\n",
    "\n",
    "#saving model\n",
    "pickle.dump(LDA_model_8_22, open( 'pickles/LDA_modelsklearn.pickle', 'wb') )\n",
    "\n",
    "#transforming to get topic numbers\n",
    "topic_values = LDA.transform(doc_term_matrix)\n",
    "topic_values.shape\n",
    "\n",
    "#creating column of Topics\n",
    "Master_Tweet_dataset['Topic'] = topic_values.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['people', 'trump', 'countries', 'public', 'health', 'coronavirus', 'american', 'covid', 'response', 'pandemic']\n",
      "Top 10 words for topic #1:\n",
      "['trumps', 'americans', 'president', 'deaths', 'pandemic', 'people', 'trump', 'covid', 'american', 'response']\n",
      "Top 10 words for topic #2:\n",
      "['especially', 'gave', 'family', 'provide', 'america', 'relief', 'service', 'american', 'covid', 'response']\n",
      "Top 10 words for topic #3:\n",
      "['support', 'trumpdeathclock', 'delayed', 'cost', 'coronavirus', 'trumps', 'lives', 'american', 'response', 'covid']\n",
      "Top 10 words for topic #4:\n",
      "['national', 'support', 'health', 'pandemic', 'thank', 'people', 'great', 'covid', 'american', 'response']\n"
     ]
    }
   ],
   "source": [
    "# top words for each topic\n",
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dictionary to create Target Column\n",
    "category_dict = {}\n",
    "for key in [0,2,3,4]:\n",
    "    category_dict[key] = 'Bad Response'\n",
    "for key in [1]:\n",
    "    category_dict[key] = 'Good Response'\n",
    "    \n",
    "#Creation of Target Column\n",
    "Master_Tweet_dataset['Target'] = Master_Tweet_dataset['Topic'].map(category_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>video</th>\n",
       "      <th>geo</th>\n",
       "      <th>positive</th>\n",
       "      <th>death</th>\n",
       "      <th>...</th>\n",
       "      <th>Poll</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>End Date</th>\n",
       "      <th>Sample</th>\n",
       "      <th>MoE</th>\n",
       "      <th>Biden (D)</th>\n",
       "      <th>Trump (R)</th>\n",
       "      <th>Spread</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1579651200000000000</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>Making great progress in @Davos. Tremendous nu...</td>\n",
       "      <td>9441</td>\n",
       "      <td>17423</td>\n",
       "      <td>87558</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ABC News/Wash PostABC/WP</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>880 RV</td>\n",
       "      <td>4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Biden +4</td>\n",
       "      <td>1</td>\n",
       "      <td>Good Response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1579651200000000000</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>Sorry, if you come you will be immediately sen...</td>\n",
       "      <td>8623</td>\n",
       "      <td>24341</td>\n",
       "      <td>98127</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ABC News/Wash PostABC/WP</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>880 RV</td>\n",
       "      <td>4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Biden +4</td>\n",
       "      <td>1</td>\n",
       "      <td>Good Response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1579651200000000000</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>See you on Friday...Big Crowd! https://twitter...</td>\n",
       "      <td>7015</td>\n",
       "      <td>24051</td>\n",
       "      <td>96738</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ABC News/Wash PostABC/WP</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>880 RV</td>\n",
       "      <td>4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Biden +4</td>\n",
       "      <td>1</td>\n",
       "      <td>Good Response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1579651200000000000</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>True! https://twitter.com/RandPaul/status/1220...</td>\n",
       "      <td>3433</td>\n",
       "      <td>11897</td>\n",
       "      <td>50187</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ABC News/Wash PostABC/WP</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>880 RV</td>\n",
       "      <td>4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Biden +4</td>\n",
       "      <td>4</td>\n",
       "      <td>Bad Response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1579651200000000000</td>\n",
       "      <td>realdonaldtrump</td>\n",
       "      <td>“NO PRESSURE”</td>\n",
       "      <td>18037</td>\n",
       "      <td>19710</td>\n",
       "      <td>121464</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ABC News/Wash PostABC/WP</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>2020-01-23</td>\n",
       "      <td>880 RV</td>\n",
       "      <td>4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Biden +4</td>\n",
       "      <td>1</td>\n",
       "      <td>Good Response</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date         username  \\\n",
       "0  1579651200000000000  realdonaldtrump   \n",
       "1  1579651200000000000  realdonaldtrump   \n",
       "2  1579651200000000000  realdonaldtrump   \n",
       "3  1579651200000000000  realdonaldtrump   \n",
       "4  1579651200000000000  realdonaldtrump   \n",
       "\n",
       "                                               tweet  replies_count  \\\n",
       "0  Making great progress in @Davos. Tremendous nu...           9441   \n",
       "1  Sorry, if you come you will be immediately sen...           8623   \n",
       "2  See you on Friday...Big Crowd! https://twitter...           7015   \n",
       "3  True! https://twitter.com/RandPaul/status/1220...           3433   \n",
       "4                                      “NO PRESSURE”          18037   \n",
       "\n",
       "   retweets_count  likes_count  video geo  positive  death  ...  \\\n",
       "0           17423        87558      0             0    NaN  ...   \n",
       "1           24341        98127      0             0    NaN  ...   \n",
       "2           24051        96738      0             0    NaN  ...   \n",
       "3           11897        50187      0             0    NaN  ...   \n",
       "4           19710       121464      0             0    NaN  ...   \n",
       "\n",
       "                       Poll  Start Date    End Date  Sample MoE Biden (D)  \\\n",
       "0  ABC News/Wash PostABC/WP  2020-01-20  2020-01-23  880 RV   4      50.0   \n",
       "1  ABC News/Wash PostABC/WP  2020-01-20  2020-01-23  880 RV   4      50.0   \n",
       "2  ABC News/Wash PostABC/WP  2020-01-20  2020-01-23  880 RV   4      50.0   \n",
       "3  ABC News/Wash PostABC/WP  2020-01-20  2020-01-23  880 RV   4      50.0   \n",
       "4  ABC News/Wash PostABC/WP  2020-01-20  2020-01-23  880 RV   4      50.0   \n",
       "\n",
       "   Trump (R)    Spread Topic         Target  \n",
       "0       46.0  Biden +4     1  Good Response  \n",
       "1       46.0  Biden +4     1  Good Response  \n",
       "2       46.0  Biden +4     1  Good Response  \n",
       "3       46.0  Biden +4     4   Bad Response  \n",
       "4       46.0  Biden +4     1  Good Response  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Master_Tweet_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving DF for use in other notebooks\n",
    "pickle.dump(Master_Tweet_dataset,open('pickles/Master_Tweet_datasetfull.pickle','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Ready DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the data fed to our models is uniform, this last step just dropped columns that were either superflous or contained text that our number-driven models cannot make any use of. For portability, the final dataframe was pickled so that it could be recalled in the Models notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns not applicable for model\n",
    "Master_Tweet_dataset = Master_Tweet_dataset.drop(['date','username','video','geo','clean_text','Start Date', \n",
    "                                                  'End Date','tweet','Sample','replies_count', 'retweets_count',\n",
    "                                                  'MoE','stopwords','Poll','MoE'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likes_count</th>\n",
       "      <th>positive</th>\n",
       "      <th>death</th>\n",
       "      <th>word_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>stopwords_count</th>\n",
       "      <th>char_count</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Biden (D)</th>\n",
       "      <th>Trump (R)</th>\n",
       "      <th>Spread</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87558</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22</td>\n",
       "      <td>5.454545</td>\n",
       "      <td>7</td>\n",
       "      <td>141</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>50.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Biden +4</td>\n",
       "      <td>1</td>\n",
       "      <td>Good Response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98127</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11</td>\n",
       "      <td>8.166667</td>\n",
       "      <td>5</td>\n",
       "      <td>109</td>\n",
       "      <td>-0.0772</td>\n",
       "      <td>50.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Biden +4</td>\n",
       "      <td>1</td>\n",
       "      <td>Good Response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96738</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>12.571429</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Biden +4</td>\n",
       "      <td>1</td>\n",
       "      <td>Good Response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50187</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>20.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>50.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Biden +4</td>\n",
       "      <td>4</td>\n",
       "      <td>Bad Response</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>121464</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>50.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>Biden +4</td>\n",
       "      <td>1</td>\n",
       "      <td>Good Response</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   likes_count  positive  death  word_count  avg_word_length  stopwords_count  \\\n",
       "0        87558         0    0.0          22         5.454545                7   \n",
       "1        98127         0    0.0          11         8.166667                5   \n",
       "2        96738         0    0.0           6        12.571429                2   \n",
       "3        50187         0    0.0           2        20.333333                0   \n",
       "4       121464         0    0.0           2         6.000000                0   \n",
       "\n",
       "   char_count  Sentiment  Biden (D)  Trump (R)    Spread  Topic         Target  \n",
       "0         141     0.7845       50.0       46.0  Biden +4      1  Good Response  \n",
       "1         109    -0.0772       50.0       46.0  Biden +4      1  Good Response  \n",
       "2          94     0.0000       50.0       46.0  Biden +4      1  Good Response  \n",
       "3          63     0.4215       50.0       46.0  Biden +4      4   Bad Response  \n",
       "4          13    -0.2960       50.0       46.0  Biden +4      1  Good Response  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Deaths didn't start immediately...filling NAN with 0s for modeling purposes\n",
    "Master_Tweet_dataset['death'] = Master_Tweet_dataset['death'].fillna(0)\n",
    "Master_Tweet_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving DF for use in other notebooks\n",
    "pickle.dump(Master_Tweet_dataset,open('pickles/Master_Tweet_dataset.pickle','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If no data has been lost, the number after this colon should be a zero : 178\n"
     ]
    }
   ],
   "source": [
    "#Confirming that no data has been lost in preprocessing\n",
    "B = len(Master_Tweet_dataset['Target'])\n",
    "print('If no data has been lost, the number after this colon should be a zero :', A-B)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Covid_notebook_7.14.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
